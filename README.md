# ğŸš€ API Data Ingestion Pipeline

### ğŸ“Œ Project Overview
This project implements a production-style API data ingestion pipeline that ingests repository metadata from the **GitHub REST API** for the Spotify organization, performs incremental updates using a metadata watermark strategy, and loads the data into **Google BigQuery** for analytics and reporting.

### ğŸ—ï¸ Architecture
**GitHub REST API** â†’ **Python Ingestion Layer** â†’ **Incremental Processing** â†’ **BigQuery**
* **Orchestration:** Apache Airflow (Dockerized)

### ğŸ§° Tech Stack
* **Language:** Python 3
* **API:** GitHub REST API v3
* **Cloud Platform:** Google Cloud Platform (GCP)
* **Data Warehouse:** BigQuery
* **Orchestration:** Apache Airflow
* **Containerization:** Docker
* **Libraries:** `requests`, `pandas`, `google-cloud-bigquery`

### ğŸ“ Repository Structure

```text
API-DATA-INGESTION-PIPELINE/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ github_ingestion_dag.py
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ service-account-key.json
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ ingestion_metadata.sql
â”‚   â””â”€â”€ repositories.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ init_metadata.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

â–¶ï¸ How to Run

    Configure environment variables: Create a .env file in the root.

    Activate Environment: source venv/bin/activate (or your local equivalent).

    Run locally:
    Bash

python src/main.py

Orchestrate with Airflow:
Bash

    docker-compose up -d

ğŸ“ˆ What I Learned

    Designing scalable API ingestion pipelines.

    Implementing incremental data processing using watermarks.

    Using BigQuery as both storage and pipeline state manager.

    Orchestrating workflows with Apache Airflow.

    Applying production-ready cloud IAM and security practices.
