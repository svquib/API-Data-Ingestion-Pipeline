ğŸ“Œ Project Overview

This project implements a production-style API data ingestion pipeline that ingests repository metadata from the GitHub REST API for the Spotify organization, performs incremental updates using a metadata watermark strategy, and loads the data into Google BigQuery for analytics and reporting.

The pipeline is designed to reflect real-world data engineering workflows, including secure authentication, pagination handling, incremental ingestion, cloud-native storage, and orchestration using Apache Airflow.

ğŸ—ï¸ Architecture

GitHub REST API â†’ Python Ingestion Layer â†’ Incremental Processing â†’ BigQuery â†’ Analytics

Orchestration: Apache Airflow (Dockerized)

ğŸ§° Tech Stack

Language: Python 3

API: GitHub REST API v3

Cloud Platform: Google Cloud Platform (GCP)

Data Warehouse: BigQuery

Orchestration: Apache Airflow

Containerization: Docker

Libraries: requests, pandas, google-cloud-bigquery

ğŸ” Key Features

API authentication using GitHub tokens

Pagination and rate-limit handling

Incremental ingestion using BigQuery metadata watermark table

Idempotent data loading into BigQuery

Airflow DAG for scheduling and retries

Secure service account-based cloud authentication

ğŸ“‚ Repository Structure
API-DATA-INGESTION-PIPELINE/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ github_ingestion_dag.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ service-account-key.json
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ ingestion_metadata.sql
â”‚   â””â”€â”€ repositories.sql
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ init_metadata.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

â–¶ï¸ How to Run

Configure environment variables (.env)

Activate Python virtual environment

Run locally:

python src/main.py

Orchestrate with Airflow:

docker compose up -d
ğŸ“Š Sample Analytics

Repository count by language

Star and fork distribution

Recently updated repositories

ğŸ“ˆ What I Learned

Designing scalable API ingestion pipelines

Implementing incremental data processing using watermarks

Using BigQuery as both storage and pipeline state manager

Orchestrating workflows with Apache Airflow

Applying production-ready cloud IAM and security practices
